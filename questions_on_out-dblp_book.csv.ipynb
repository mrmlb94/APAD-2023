{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's take this step by step. We'll start with Question 1 for the new dataset (out-dblp_book.csv).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1, Part A: Which is the publication with the largest number of authors? Report Title and Authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/_cd9dbj914lbnxxjt6s0_7000000gn/T/ipykernel_1582/2556290384.py:5: DtypeWarning: Columns (2,5,6,7,13,18,19,20,23,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"out-dblp_book.csv\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset (e.g., out-dblp_book.csv)\n",
    "data = pd.read_csv(\"out-dblp_book.csv\", sep=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "authors = data['author-bibtex'].str.split('|')\n",
    "publications = data['title']\n",
    "\n",
    "# Create a bipartite graph with authors and publications as nodes\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add authors as nodes\n",
    "for author_list in authors:\n",
    "    if isinstance(author_list, list):  # Check if it's a list (not NaN)\n",
    "        for author in author_list:\n",
    "            G.add_node(author, bipartite=0)\n",
    "\n",
    "# Add publications as nodes\n",
    "for title in publications:\n",
    "    G.add_node(title, bipartite=1)\n",
    "\n",
    "# Add edges connecting authors to publications\n",
    "for i, author_list in enumerate(authors):\n",
    "    if isinstance(author_list, list):  # Check if it's a list (not NaN)\n",
    "        for author in author_list:\n",
    "            G.add_edge(author, publications[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the set of publications from the graph\n",
    "publications = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 1}\n",
    "\n",
    "# Initialize variables to keep track of the publication with the most authors\n",
    "max_authors_publication = None\n",
    "max_author_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through publications and count the number of authors\n",
    "for publication in publications:\n",
    "    authors = list(G.neighbors(publication))\n",
    "    author_count = len(authors)\n",
    "    if author_count > max_author_count:\n",
    "        max_authors_publication = publication\n",
    "        max_author_count = author_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The publication with the largest number of authors is 'Homotopy Type Theory: Univalent Foundations of Mathematics.' with 1 authors.\n"
     ]
    }
   ],
   "source": [
    "# Report the title and authors of the publication with the largest number of authors\n",
    "print(f\"The publication with the largest number of authors is '{max_authors_publication}' with {max_author_count} authors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/mrmlb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most used words in titles of component with at least 30 publications: [('-', 2), ('und', 2), ('Programmierungsanleitung', 1), ('Z', 1), ('22', 1), ('Zuse', 1), ('K.-G.', 1), ('Neukirchen,', 1), ('Kreis', 1), ('HÃ¼nfeld:', 1)]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import nltk  # Import nltk here\n",
    "\n",
    "# Download the NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n",
    "# Load the new dataset (e.g., out-dblp_book.csv)\n",
    "data = pd.read_csv(\"out-dblp_book.csv\", sep=\";\", low_memory=False)  # Suppress the dtype warning\n",
    "\n",
    "# Data Preprocessing\n",
    "# Assuming the 'year' column contains publication years, clean it to extract the first four digits\n",
    "data['year'] = data['year'].str.extract(r'(\\d{4})').astype(float).fillna(0).astype(int)\n",
    "\n",
    "# Create a bipartite graph with authors and publications as nodes\n",
    "G = nx.Graph()\n",
    "authors = set()\n",
    "publications = set()\n",
    "\n",
    "# Iterate through the dataset and add nodes and edges to the graph\n",
    "for index, row in data.iterrows():\n",
    "    authors_list = str(row['author']).split('|')\n",
    "    publication = row['title']\n",
    "    year = row['year']\n",
    "\n",
    "    # Add authors as nodes\n",
    "    for author in authors_list:\n",
    "        if author:  # Check for missing values\n",
    "            G.add_node(author, bipartite=0)\n",
    "            authors.add(author)\n",
    "\n",
    "    # Add publications as nodes\n",
    "    G.add_node(publication, bipartite=1, year=year)\n",
    "    publications.add(publication)\n",
    "\n",
    "    # Add edges between authors and publications\n",
    "    for author in authors_list:\n",
    "        if author:\n",
    "            G.add_edge(author, publication)\n",
    "\n",
    "# Define the target year (e.g., 1970)\n",
    "target_year = 1970\n",
    "\n",
    "# Filter publications by year (up to the target year)\n",
    "filtered_publications = {publication for publication in publications if G.nodes[publication]['year'] <= target_year}\n",
    "\n",
    "# Get the largest connected component\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "largest_cc_graph = G.subgraph(largest_cc)\n",
    "\n",
    "# Iterate through the components in the largest connected component\n",
    "for component in nx.connected_components(largest_cc_graph):\n",
    "    if len(component) >= 30:\n",
    "        # Extract the titles of publications in this component\n",
    "        component_titles = [publication for publication in component if publication in filtered_publications]\n",
    "\n",
    "        # Tokenize the titles, exclude stopwords, and count word frequencies\n",
    "        stopwords_list = set(stopwords.words(\"english\"))\n",
    "        word_frequencies = Counter()\n",
    "        for title in component_titles:\n",
    "            words = title.split()\n",
    "            cleaned_words = [word for word in words if word.lower() not in stopwords_list]\n",
    "            word_frequencies.update(cleaned_words)\n",
    "\n",
    "        # Find the most used words in titles\n",
    "        most_used_words = word_frequencies.most_common(10)\n",
    "        print(f\"Most used words in titles of component with at least 30 publications: {most_used_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/_cd9dbj914lbnxxjt6s0_7000000gn/T/ipykernel_1582/2949169423.py:5: DtypeWarning: Columns (2,5,6,7,13,18,19,20,23,33) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"out-dblp_book.csv\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no authors with collaborations for the target year in the dataset.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Load the new dataset (e.g., out-dblp_book.csv)\n",
    "data = pd.read_csv(\"out-dblp_book.csv\", sep=\";\")\n",
    "\n",
    "# Data Preprocessing\n",
    "# Extract the year from the 'year' column and handle missing or non-numeric values\n",
    "data['year'] = data['year'].str.extract(r'(\\d{4})').fillna(0).astype(int)\n",
    "\n",
    "# Create a bipartite graph with authors and publications as nodes\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate through the dataset and add nodes and edges to the graph\n",
    "for index, row in data.iterrows():\n",
    "    authors_list = str(row['author']).split('|')  # Convert to string to handle NaN values\n",
    "    publication = row['title']\n",
    "    year = int(row['year'])\n",
    "\n",
    "    # Add nodes for authors and publication\n",
    "    G.add_nodes_from(authors_list, bipartite=0)\n",
    "    G.add_node(publication, bipartite=1)\n",
    "\n",
    "    # Add edges between authors and publication\n",
    "    G.add_edges_from([(author, publication) for author in authors_list])\n",
    "\n",
    "# Specify the target year (e.g., 1970)\n",
    "target_year = 1970\n",
    "\n",
    "# Filter publications by the target year\n",
    "filtered_publications = [publication for publication, year in zip(G.nodes, nx.get_node_attributes(G, 'year').values()) if year <= target_year]\n",
    "\n",
    "# Create a list of unique authors who contributed to these publications\n",
    "collaborators = set()\n",
    "for publication in filtered_publications:\n",
    "    authors = set(G.neighbors(publication))\n",
    "    collaborators.update(authors)\n",
    "\n",
    "# Check if there are authors with collaborations\n",
    "if collaborators:\n",
    "    # Count the number of collaborations for each author\n",
    "    collaboration_counts = {}\n",
    "    for author in collaborators:\n",
    "        collaborations = set(G.neighbors(author))\n",
    "        for publication in filtered_publications:\n",
    "            if publication in collaborations:\n",
    "                collaborations.remove(publication)  # Remove author's own publications\n",
    "        collaboration_counts[author] = len(collaborations)\n",
    "\n",
    "    # Identify the author with the largest number of collaborations\n",
    "    most_collaborative_author = max(collaboration_counts, key=collaboration_counts.get)\n",
    "    max_collaborations = collaboration_counts[most_collaborative_author]\n",
    "\n",
    "    print(f\"The author with the largest number of collaborations up to {target_year} is {most_collaborative_author} with {max_collaborations} collaborations.\")\n",
    "else:\n",
    "    print(\"There are no authors with collaborations for the target year in the dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It appears that the code executed without any errors, and it indicates that there are no authors with collaborations for the target year in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
